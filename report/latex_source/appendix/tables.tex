\chapter{Tables}

\begin{table}[h!]
    \centering
    \caption{LSTM hyperparameters ranges and their optimal values.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ || c | c | c | c || }
        \hline
        Hyperparameter & Range & Optimal value & Description \Tstrut\Bstrut \\
        \hline
        n\_layers & \lstmParamsRangeNlayers & \lstmParamsNlayers & number of layers \Tstrut\\[0.5ex]
        last\_units  & \lstmParamsRangeNunits & \lstmParamsNunits & number of nodes for the last hidden layer \\[0.5ex]
        units\_decay & \lstmParamsRangeScale & \lstmParamsScale & decay rate of the number of nodes \\[0.5ex]
        learning\_rate & \lstmParamsRangeLR & \lstmParamsLR & learning rate of the model \\[0.5ex]
        lr\_decay & \lstmParamsRangeDecay & \lstmParamsDecay & decay rate of the learning rate \\[0.5ex]
        dropout\_rate & \lstmParamsRangeDropout & \lstmParamsDropout & dropout rate for the dropout layers \\[0.5ex]
        norm & \lstmParamsRangeNorm & \lstmParamsNorm & whether to add batch normalization layers \\[0.5ex]
        batch\_size & \lstmParamsRangeBatch & \lstmParamsBatch & batch size for the training phase \\[0.5ex]
        steps & \lstmParamsRangeSteps & \lstmParamsSteps & number of time steps for LSTM data \\[0.5ex]
        \hline
    \end{tabular}}
    \label{tab:lstm_params}
\end{table}

\begin{table}[h!]
    \centering
    \caption{MLP hyperparameters ranges and their optimal values.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ || c | c | c | c || }
        \hline
        Hyperparameter & Range & Optimal value & Description \Tstrut\Bstrut \\
        \hline
        n\_layers & \annParamsRangeNlayers & \annParamsNlayers & number of layers \Tstrut\\[0.5ex]
        last\_units  & \annParamsRangeNunits & \annParamsNunits & number of nodes for the last hidden layer \\[0.5ex]
        units\_decay & \annParamsRangeScale & \annParamsScale & decay rate of the number of nodes \\[0.5ex]
        learning\_rate & \annParamsRangeLR & \annParamsLR & learning rate of the model \\[0.5ex]
        lr\_decay & \annParamsRangeDecay & \annParamsDecay & decay rate of the learning rate \\[0.5ex]
        dropout\_rate & \annParamsRangeDropout & \annParamsDropout & dropout rate for the dropout layers \\[0.5ex]
        norm & \annParamsRangeNorm & \annParamsNorm & whether to add batch normalization layers \\[0.5ex]
        batch\_size & \annParamsRangeBatch & \annParamsBatch & batch size for the training phase \\[0.5ex]
        \hline
    \end{tabular}}
    \label{tab:ann_params}
\end{table}

\begin{table}[h!]
    \centering
    \caption{LGBM hyperparameters ranges and their optimal values.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ || c | c | c | c || }
        \hline
        Hyperparameter & Range & Optimal value & Description \Tstrut\Bstrut \\
        \hline
        learning\_rate & \lgbmParamsRangeLR & \lgbmParamsLR & learning rate of the model \Tstrut\\[0.5ex]
        feature\_fraction & \lgbmParamsRangeFeatFrac & \lgbmParamsFeatFrac & fraction of features used to train each tree \\[0.5ex]
        lambda\_l2 & \lgbmParamsRangeLambda & \lgbmParamsLambda & value of lambda for L2 regularization \\[0.5ex]
        num\_leaves & \lgbmParamsRangeNleaves & \lgbmParamsNleaves & maximum number of leaves in one tree \\[0.5ex]
        min\_data\_in\_leaf & \lgbmParamsRangeMinData & \lgbmParamsMinData & Minimum number of data in each leaf \\[0.5ex]
        \hline
    \end{tabular}}
    \label{tab:lgbm_params}
\end{table}